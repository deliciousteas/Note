# 图神经网络

## 什么是图，什么是图神经网络？

图是一种由节点和边组成的数据结构，关注非欧氏几何空间，non-Euclidean。

GNN（Graph neural networks）是基于图领域的一种深度学习方法的总称。

**1997-2009**

**典型方法：**引入递归神经网络和前馈神经网络，思想是在图上传递状态，迭代到收敛。

* 递归神经网络（RNN）：具有树状阶层结构且网络节点按顺序对`INPUT`递归处理的人工神经网络（ANN）

* 前馈神经网络（feedforward neural network，FNN）：整个网络无反馈，单线流通。可以将神经网络理解为输入层、隐层、输出层。

**原因：**

1. 递归。递归对深层复杂关系难以满足，数据类型和物理条件约束。
2. 局部上下文。神经网络在状态传递时，通常只考虑到局部上下文，无法直接捕获到全局的上下文关系。
3. 迭代收敛。收敛性和稳定性在大规模数据面临困难，且受到初始状态和参数设置影响。

**1998：CNNs的发现使得GNN重新受到关注**

杨立坤最早提出CNNs，CNNS可以提取多尺度局部空间信息，并且能高度浓缩表达能力。

CNN的关键在于局部链接、权值共享、多层连接。然而CNN主要解决基于欧氏空间

**Graph representation learning**

in graph analysis,主要集中于手工打标签（监督学习），高成本和不灵活。

受到Graph representation learning和word-embedding的启发，图嵌入成为一种新的方法。

**graph embedding**

然而这种方法并没有考虑节点之间共享参数，导致计算量随着节点数量线性增长。其次嵌入方法缺少泛化能力。，动态图和新图得不到。

节点之间的连接关系可能非常复杂且多样化，在大规模图中，连接属性和关系的考虑会导致模型的复杂性和计算成本的增加。

**图神经网络(GNNS)**

**cnn**+**graph embedding**思想的基础上，提出图神经网络。对图结构中的信息总体聚合。

其中结合路网最相关的卷积神经网络和图自编码器（c**onvolutional graph neural networks,graph autoencoders**),分别适用于局部特征提取，图的表示学习.

# 备注

收敛：损失函数的值逐渐接近最小值。

损失函数：预测输出结果与实际输出结果差异的度量。

鲁棒性：具有较高精度和有效性

Regularization正则化：防止过拟合，简化特征提取。

反向传播：ANN的一种训练模型算法，计算梯度确定误差对于网络中每个参数的影响程度。

梯度：损失函数在神经网络中的变化方向。通过梯度，我们可以知道如何改变网络的权重和偏置，以使损失函数最小化。 梯度法是一种优化算法，用于寻找损失函数的最小值。

局部链接：每个节点之和前一层的局部区域连接。

卷积：filter滤波器

权值共享：同一卷积层

低维：只保留有用信息，简化学习任务。

注意力机制：深度学习中，模拟人类的注意力分配，就是对input层不同部分赋予不同权重。

词嵌入：将词语分类，自定义分类的一个词典，将词语映射到实数向量方法的总称。

encoder：编码器，学习数据的低维特征，实现高效的特征提取和数据压缩。

graph embedding：将图数据，映射为低稠密向量的过程。

# 参考：

[递归神经网络](https://baike.baidu.com/item/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/16020230)

[前馈神经网络](https://baike.baidu.com/item/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7580523)

[损失函数](https://cloud.tencent.com/developer/article/2322229)

[鲁棒性](https://blog.csdn.net/weixin_44222014/article/details/103565351)

[反向传播](https://www.amazonaws.cn/en/knowledge/what-is-backpropagation/#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD(Backpropagation)%20%E6%98%AF,%E5%8F%82%E6%95%B0%E5%AF%B9%E8%AF%AF%E5%B7%AE%E7%9A%84%E8%B4%A1%E7%8C%AE%E3%80%82)

[什么是高维低维数据](https://www.volcengine.com/theme/1207528-S-7-1#:~:text=%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%EF%BC%8C%E6%95%B0%E6%8D%AE,%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89%E3%80%82)

[监督学习](https://blog.csdn.net/qq_44015059/article/details/106448533)

[表征学习，Representation learning](https://paperswithcode.com/task/representation-learning)

[注意力机制：人和深度学习上的差异](https://zhuanlan.zhihu.com/p/379722366)

[什么是词嵌入word embedding](https://blog.csdn.net/qq_39521554/article/details/86696202)

[什么是图嵌入](https://zhuanlan.zhihu.com/p/87572912)

[Graph neural networks: A review of methods and applications](https://www.sciencedirect.com/science/article/pii/S2666651021000012)

[图计算入门：高配版](https://www.51cto.com/article/742570.html)

[空间同质性](https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E5%90%8C%E8%B4%A8%E6%80%A7)

[如何比较基于谱和基于空间的区别](https://blog.csdn.net/qq_44689178/article/details/123396737)

[几种典型图神经网络原理讲解](https://zhuanlan.zhihu.com/p/136521625)

[从图到图卷积：漫谈图神经网络](https://github.com/SivilTaram/Graph-Neural-Network-Note)

[图神经网络几种类型简要介绍](https://www.cnblogs.com/xine/p/14985581.html)

